{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB Reviews NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KenVe8SAfutp"
      },
      "source": [
        "**Get Data From Here** :- <a href=\"https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv\">Get Data</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am7hnzTPgUDu",
        "outputId": "0ffeacf7-2667-4e33-84c8-2e818b739cb9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4HckEOufutq"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9JGoiztfutq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw8Xgmy9futr"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "Co255Y-Ufutr",
        "outputId": "4d0b6bf1-e22a-4060-eff2-c421868987b5"
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/IMDB Dataset.csv')\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5  Probably my all-time favorite movie, a story o...  positive\n",
              "6  I sure would like to see a resurrection of a u...  positive\n",
              "7  This show was an amazing, fresh & innovative i...  negative\n",
              "8  Encouraged by the positive comments about this...  negative\n",
              "9  If you like original gut wrenching laughter yo...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "PRV01eWLfuts",
        "outputId": "6c10601f-7248-427a-c627-d14854e7adab"
      },
      "source": [
        "print(\"Summary statistics of numerical features : \\n\", df.describe())\n",
        "\n",
        "print(\"=======================================================================\")\n",
        "\n",
        "print(\"\\nTotal number of reviews: \",len(df))\n",
        "\n",
        "print(\"=======================================================================\")\n",
        "\n",
        "print(\"\\nTotal number of Sentiments: \", len(list(set(df['sentiment']))))\n",
        "\n",
        "df['sentiment'] = np.where(df['sentiment'] == \"positive\", 1, 0)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary statistics of numerical features : \n",
            "                                                    review sentiment\n",
            "count                                               50000     50000\n",
            "unique                                              49582         2\n",
            "top     Loved today's show!!! It was a variety and not...  negative\n",
            "freq                                                    5     25000\n",
            "=======================================================================\n",
            "\n",
            "Total number of reviews:  50000\n",
            "=======================================================================\n",
            "\n",
            "Total number of Sentiments:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review  sentiment\n",
              "0      One of the other reviewers has mentioned that ...          1\n",
              "1      A wonderful little production. <br /><br />The...          1\n",
              "2      I thought this was a wonderful way to spend ti...          1\n",
              "3      Basically there's a family where a little boy ...          0\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...          1\n",
              "...                                                  ...        ...\n",
              "49995  I thought this movie did a down right good job...          1\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
              "49997  I am a Catholic taught in parochial elementary...          0\n",
              "49998  I'm going to have to disagree with the previou...          0\n",
              "49999  No one expects the Star Trek movies to be high...          0\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1GDx0Rpfuts"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "NzY0NaTNfuts",
        "outputId": "61c42ad2-8985-4a67-d8df-fa584a58e718"
      },
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "# sns.countplot(df['Rating'])\n",
        "df['sentiment'].value_counts().sort_index().plot(kind='bar',color = 'blue')\n",
        "plt.title('Distribution of Rating')\n",
        "plt.grid()\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "ax = plt.axes()\n",
        "ax.set_facecolor(\"white\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhCLSXRqfutu"
      },
      "source": [
        "# Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfVYwo75futu",
        "outputId": "490e616c-c098-4a21-c04b-1dd865970cc3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], \\\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n",
        "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n",
        "print('Show a review in the training set : \\n', X_train.iloc[10])\n",
        "X_train,y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load 40000 training examples and 10000 validation examples. \n",
            "\n",
            "Show a review in the training set : \n",
            " Add pure humor + quick and unique sentences + sex + unfaith sex! + love + lies + dark deadly thoughts + secret plans + fun + black humor + sex!.. again! + black dresses! (needed for the unlimited funerals!) = Eglimata!!! Or in English, Crimes!! Our Heroes are two married couples, their relatives, their friends and neighbors. There is Soso and Alekos and Flora and Achilleas, two married couples who have everything but not real love! Flora is the mistress of Alekos, and when Soso finds what's going on, she is planning with her best friend Pepi to kill Alekos and look like an accident! Many plans were made but everyone else dies except Alekos! Achilleas find's out that he has a sister who is a Hooker and tries to put her in the right road..Korina is a temptation to mens but her tries to get married all goes wrong, since when they learn her past, freaks and leave and she ends up marrying a rich farm man. As for the other roles they are like they are from Cartoons! Grandpa Aristidis which fakes that he is paralyzed, Machi is his nurse who is secretly marry to Aristidis for his fortune, Johny, son of Machi, who has it OK with everybody to have all the benefits, Michalakis who has only one purpose in life.. to suicide, but he is unable to do it so he is desperate! Every time, I see the replays and every time when it finishes I miss it.. One of my favorite All time classics...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20330    That's about the only redeeming quality in a m...\n",
              " 17532    Even if I had not read Anne Rice's \"Queen of t...\n",
              " 45819    I sort of liked this Columbo movie its atmosph...\n",
              " 34807    \"Zabriskie Point\" (1970): This was especially ...\n",
              " 31888    Quite one of the worst films I have ever seen....\n",
              "                                ...                        \n",
              " 21243    I did not set very high expectations for this ...\n",
              " 45891    THE BLOB is a great horror movie, not merely b...\n",
              " 42613    After too many years of waiting, Anne Rivers S...\n",
              " 43567    I am a massive fan of the LoG. I thought the f...\n",
              " 2732     AG was an excellent presentation of drama, sus...\n",
              " Name: review, Length: 40000, dtype: object, 20330    0\n",
              " 17532    0\n",
              " 45819    1\n",
              " 34807    1\n",
              " 31888    0\n",
              "         ..\n",
              " 21243    1\n",
              " 45891    1\n",
              " 42613    1\n",
              " 43567    0\n",
              " 2732     1\n",
              " Name: sentiment, Length: 40000, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7nb7qZRfutu"
      },
      "source": [
        "# Bag of Words\n",
        "<br>\n",
        "\n",
        "**Step 1 : Preprocess raw reviews to cleaned reviews**\n",
        "\n",
        "**Step 2 : Create BoW using CountVectorizer / Tfidfvectorizer in sklearn**\n",
        "\n",
        "**Step 3 : Transform review text to numerical representations (feature vectors)**\n",
        "\n",
        "**Step 4 : Fit feature vectors to supervised learning algorithm (eg. Naive Bayes, Logistic regression, etc.)**\n",
        "\n",
        "**Step 5 : Improve the model performance by GridSearch**\n",
        "\n",
        "# Text Preprocessing\n",
        "<br>\n",
        "\n",
        "**Step 1 : remove html tags using BeautifulSoup**\n",
        "\n",
        "**Step 2 : remove non-character such as digits and symbols**\n",
        "\n",
        "**Step 3 : convert to lower case**\n",
        "\n",
        "**Step 4 : remove stop words such as \"the\" and \"and\" if needed**\n",
        "\n",
        "**Step 5 : convert to root words by stemming if needed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlST1wE8futv"
      },
      "source": [
        "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
        "             ):\n",
        "    '''\n",
        "    Convert a raw review to a cleaned review\n",
        "    '''\n",
        "    text = BeautifulSoup(raw_text, 'html.parser').get_text()\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    words = letters_only.lower().split() \n",
        "    \n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "        \n",
        "    if stemming==True:\n",
        "\n",
        "        stemmer = SnowballStemmer('english') \n",
        "        words = [stemmer.stem(w) for w in words]\n",
        "        \n",
        "    if split_text==True:\n",
        "        return (words)\n",
        "    \n",
        "    return( \" \".join(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unFrRXR8futv",
        "outputId": "00dcd46c-44a4-4ae6-e5b8-b4b935882da7"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "from bs4 import BeautifulSoup \n",
        "import logging\n",
        "from wordcloud import WordCloud\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "X_train_cleaned = []\n",
        "X_test_cleaned = []\n",
        "\n",
        "for d in X_train:\n",
        "    X_train_cleaned.append(cleanText(d))\n",
        "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
        "    \n",
        "for d in X_test:\n",
        "    X_test_cleaned.append(cleanText(d))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Show a cleaned review in the training set : \n",
            " add pure humor quick and unique sentences sex unfaith sex love lies dark deadly thoughts secret plans fun black humor sex again black dresses needed for the unlimited funerals eglimata or in english crimes our heroes are two married couples their relatives their friends and neighbors there is soso and alekos and flora and achilleas two married couples who have everything but not real love flora is the mistress of alekos and when soso finds what s going on she is planning with her best friend pepi to kill alekos and look like an accident many plans were made but everyone else dies except alekos achilleas find s out that he has a sister who is a hooker and tries to put her in the right road korina is a temptation to mens but her tries to get married all goes wrong since when they learn her past freaks and leave and she ends up marrying a rich farm man as for the other roles they are like they are from cartoons grandpa aristidis which fakes that he is paralyzed machi is his nurse who is secretly marry to aristidis for his fortune johny son of machi who has it ok with everybody to have all the benefits michalakis who has only one purpose in life to suicide but he is unable to do it so he is desperate every time i see the replays and every time when it finishes i miss it one of my favorite all time classics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0nSgc2Afutv"
      },
      "source": [
        "## CountVectorizer with Multinomial Naive Bayes (Benchmark Model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clN27WF6futw",
        "outputId": "dc291ad1-5a18-4d70-a2f3-99fde08a7bf3"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "countVect = CountVectorizer() \n",
        "X_train_countVect = countVect.fit_transform(X_train_cleaned)\n",
        "print(\"Number of features : %d \\n\" %len(countVect.get_feature_names())) #6378 \n",
        "print(\"Show some feature names : \\n\", countVect.get_feature_names()[::1000])\n",
        "\n",
        "\n",
        "# Train MultinomialNB classifier\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_countVect, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features : 92259 \n",
            "\n",
            "Show some feature names : \n",
            " ['aa', 'ads', 'allegorical', 'ankh', 'army', 'austrians', 'bankable', 'behalf', 'biplanes', 'bolliwood', 'brennaman', 'burnstyn', 'carabiners', 'cermonies', 'choco', 'clothers', 'complicates', 'convientantly', 'creds', 'dads', 'deeriving', 'desperado', 'discharges', 'dominick', 'dulhan', 'ekes', 'entailed', 'ewen', 'falsetto', 'fieriest', 'flushes', 'frickin', 'garvin', 'gladiator', 'granting', 'gutwrenching', 'hasta', 'heyy', 'hopper', 'icp', 'indefensible', 'interiorized', 'jailbreak', 'judgments', 'khakee', 'kristevian', 'laundry', 'likable', 'lousy', 'maids', 'martyn', 'medicines', 'milbank', 'moltisanti', 'mugged', 'naxalites', 'nolonger', 'oedipial', 'otherness', 'pandora', 'pegged', 'physics', 'pointillistic', 'preeners', 'provisos', 'qwak', 'rebarba', 'remarriage', 'rewords', 'roomful', 'sales', 'schlegel', 'senate', 'shikakai', 'sinned', 'snapper', 'sparklingly', 'standards', 'streetlights', 'superdupercharged', 'tabletop', 'tempo', 'throwback', 'toschi', 'trojen', 'unalterably', 'unmade', 'valerio', 'virgil', 'waterboard', 'wikpedia', 'writings', 'zesty']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZX4QBRFfutw"
      },
      "source": [
        "pickle.dump(countVect,open('countVect_imdb.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbIx5_HRfutx"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "def modelEvaluation(predictions):\n",
        "    '''\n",
        "    Print model evaluation to predicted result \n",
        "    '''\n",
        "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
        "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n",
        "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
        "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M09plMgifutx",
        "outputId": "fe516380-31e0-40b8-eff2-17316dd32806"
      },
      "source": [
        "predictions = mnb.predict(countVect.transform(X_test_cleaned))\n",
        "modelEvaluation(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on validation set: 0.8450\n",
            "\n",
            "AUC score : 0.8448\n",
            "\n",
            "Classification report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85      5035\n",
            "           1       0.87      0.81      0.84      4965\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.85      0.84      0.84     10000\n",
            "weighted avg       0.85      0.84      0.84     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            " [[4419  616]\n",
            " [ 934 4031]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8eVay8hfutx"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(mnb,open('Naive_Bayes_model_imdb.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxF4wZ7ffutx"
      },
      "source": [
        "# TfidfVectorizer with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLI9_UTHfuty",
        "outputId": "60711358-e348-4f7b-a6c4-bc3f74cd1cf8"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\n",
        "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features : 33686 \n",
            "\n",
            "Show some feature names : \n",
            " ['00', 'aishwarya', 'assigns', 'bengal', 'brining', 'challenge', 'commence', 'creatively', 'depend', 'doped', 'enforces', 'fastest', 'freleng', 'grandparents', 'heralded', 'implementation', 'izzy', 'lain', 'luke', 'mencia', 'mutual', 'onwards', 'penultimate', 'predecessors', 'ramón', 'retelling', 'saving', 'showcasing', 'spanish', 'submachine', 'teutonic', 'trying', 'vanished', 'wig']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KatgeRIZfuty",
        "outputId": "8f096519-dce5-4f14-ecb5-8bb38aedc1c7"
      },
      "source": [
        "feature_names = np.array(tfidf.get_feature_names())\n",
        "sorted_coef_index = lr.coef_[0].argsort()\n",
        "print('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 features with smallest coefficients :\n",
            "['worst' 'bad' 'waste' 'awful' 'boring' 'terrible' 'poor' 'nothing' 'dull'\n",
            " 'horrible']\n",
            "\n",
            "Top 10 features with largest coefficients : \n",
            "['great' 'excellent' 'best' 'perfect' 'wonderful' 'amazing' 'loved'\n",
            " 'today' 'fun' 'favorite']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2jD3rkMfuty",
        "outputId": "faafa107-b518-4047-8a23-867aa56406e4"
      },
      "source": [
        "predictions = lr.predict(tfidf.transform(X_test_cleaned))\n",
        "modelEvaluation(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on validation set: 0.8906\n",
            "\n",
            "AUC score : 0.8907\n",
            "\n",
            "Classification report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89      5035\n",
            "           1       0.88      0.90      0.89      4965\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            " [[4426  609]\n",
            " [ 485 4480]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORg6ZE_bfutz",
        "outputId": "2d918d6f-e14f-41f5-eceb-75c0b91fb95f"
      },
      "source": [
        "from sklearn.model_selection import  GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "estimators = [(\"tfidf\", TfidfVectorizer()), (\"lr\", LogisticRegression())]\n",
        "model = Pipeline(estimators)\n",
        "\n",
        "\n",
        "params = {\"lr__C\":[0.1, 1, 10], \n",
        "          \"tfidf__min_df\": [1, 3], \n",
        "          \"tfidf__max_features\": [1000, None], \n",
        "          \"tfidf__ngram_range\": [(1,1), (1,2)], \n",
        "          \"tfidf__stop_words\": [None, \"english\"]} \n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid.fit(X_train_cleaned, y_train)\n",
        "print(\"The best paramenter set is : \\n\", grid.best_params_)\n",
        "\n",
        "\n",
        "# Evaluate on the validaton set\n",
        "predictions = grid.predict(X_test_cleaned)\n",
        "modelEvaluation(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best paramenter set is : \n",
            " {'lr__C': 10, 'tfidf__max_features': None, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None}\n",
            "\n",
            "Accuracy on validation set: 0.9090\n",
            "\n",
            "AUC score : 0.9090\n",
            "\n",
            "Classification report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91      5035\n",
            "           1       0.91      0.91      0.91      4965\n",
            "\n",
            "    accuracy                           0.91     10000\n",
            "   macro avg       0.91      0.91      0.91     10000\n",
            "weighted avg       0.91      0.91      0.91     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            " [[4563  472]\n",
            " [ 438 4527]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbNErqSOfutz"
      },
      "source": [
        "# Word2Vec\n",
        "<br>\n",
        "\n",
        "**Step 1 : Parse review text to sentences (Word2Vec model takes a list of sentences as inputs)**\n",
        "\n",
        "**Step 2 : Create volcabulary list using Word2Vec model.**\n",
        "\n",
        "**Step 3 : Transform each review into numerical representation by computing average feature vectors of words therein.**\n",
        "\n",
        "**Step 4 : Fit the average feature vectors to Random Forest Classifier.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PujT6B3Lfutz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8e1a41-7e79-48c8-fc0c-39557c9e6a4f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def parseSent(review, tokenizer, remove_stopwords=False):\n",
        "\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# Parse each review in the training set into sentences\n",
        "sentences = []\n",
        "for review in X_train_cleaned:\n",
        "    sentences += parseSent(review, tokenizer,remove_stopwords=False)\n",
        "    \n",
        "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
        "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "40000 parsed sentence in the training set\n",
            "\n",
            "Show a parsed sentence in the training set : \n",
            " ['add', 'pure', 'humor', 'quick', 'and', 'unique', 'sentences', 'sex', 'unfaith', 'sex', 'love', 'lies', 'dark', 'deadly', 'thoughts', 'secret', 'plans', 'fun', 'black', 'humor', 'sex', 'again', 'black', 'dresses', 'needed', 'for', 'the', 'unlimited', 'funerals', 'eglimata', 'or', 'in', 'english', 'crimes', 'our', 'heroes', 'are', 'two', 'married', 'couples', 'their', 'relatives', 'their', 'friends', 'and', 'neighbors', 'there', 'is', 'soso', 'and', 'alekos', 'and', 'flora', 'and', 'achilleas', 'two', 'married', 'couples', 'who', 'have', 'everything', 'but', 'not', 'real', 'love', 'flora', 'is', 'the', 'mistress', 'of', 'alekos', 'and', 'when', 'soso', 'finds', 'what', 's', 'going', 'on', 'she', 'is', 'planning', 'with', 'her', 'best', 'friend', 'pepi', 'to', 'kill', 'alekos', 'and', 'look', 'like', 'an', 'accident', 'many', 'plans', 'were', 'made', 'but', 'everyone', 'else', 'dies', 'except', 'alekos', 'achilleas', 'find', 's', 'out', 'that', 'he', 'has', 'a', 'sister', 'who', 'is', 'a', 'hooker', 'and', 'tries', 'to', 'put', 'her', 'in', 'the', 'right', 'road', 'korina', 'is', 'a', 'temptation', 'to', 'mens', 'but', 'her', 'tries', 'to', 'get', 'married', 'all', 'goes', 'wrong', 'since', 'when', 'they', 'learn', 'her', 'past', 'freaks', 'and', 'leave', 'and', 'she', 'ends', 'up', 'marrying', 'a', 'rich', 'farm', 'man', 'as', 'for', 'the', 'other', 'roles', 'they', 'are', 'like', 'they', 'are', 'from', 'cartoons', 'grandpa', 'aristidis', 'which', 'fakes', 'that', 'he', 'is', 'paralyzed', 'machi', 'is', 'his', 'nurse', 'who', 'is', 'secretly', 'marry', 'to', 'aristidis', 'for', 'his', 'fortune', 'johny', 'son', 'of', 'machi', 'who', 'has', 'it', 'ok', 'with', 'everybody', 'to', 'have', 'all', 'the', 'benefits', 'michalakis', 'who', 'has', 'only', 'one', 'purpose', 'in', 'life', 'to', 'suicide', 'but', 'he', 'is', 'unable', 'to', 'do', 'it', 'so', 'he', 'is', 'desperate', 'every', 'time', 'i', 'see', 'the', 'replays', 'and', 'every', 'time', 'when', 'it', 'finishes', 'i', 'miss', 'it', 'one', 'of', 'my', 'favorite', 'all', 'time', 'classics']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t3kkyRzfutz"
      },
      "source": [
        "## Creating Volcabulary List using Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OZO0vBAfut0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc154809-e2d4-4b99-99c3-576a0d9894c7"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from gensim.models import word2vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "num_features = 300  #embedding dimension                     \n",
        "min_word_count = 10                \n",
        "num_workers = 4       \n",
        "context = 10                                                                                          \n",
        "downsampling = 1e-3 \n",
        "\n",
        "print(\"Training Word2Vec model ...\\n\")\n",
        "w2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,\\\n",
        "                 window = context, sample = downsampling)\n",
        "w2v.init_sims(replace=True)\n",
        "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n",
        "\n",
        "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) #4016 \n",
        "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Word2Vec model ...\n",
            "\n",
            "Number of words in the vocabulary list : 25087 \n",
            "\n",
            "Show first 10 words in the vocalbulary list  vocabulary list: \n",
            " ['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZdhuOnwfut0"
      },
      "source": [
        "## Averaging Feature Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7KTHEUkfut0"
      },
      "source": [
        "def makeFeatureVec(review, model, num_features):\n",
        "    '''\n",
        "    Transform a review to a feature vector by averaging feature vectors of words \n",
        "    appeared in that review and in the volcabulary list created\n",
        "    '''\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    nwords = 0.\n",
        "    index2word_set = set(model.wv.index2word) #index2word is the volcabulary list of the Word2Vec model\n",
        "    isZeroVec = True\n",
        "    for word in review:\n",
        "        if word in index2word_set: \n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec, model[word])\n",
        "            isZeroVec = False\n",
        "    if isZeroVec == False:\n",
        "        featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TizrMzLtfut1"
      },
      "source": [
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    '''\n",
        "    Transform all reviews to feature vectors using makeFeatureVec()\n",
        "    '''\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
        "        counter = counter + 1\n",
        "    return reviewFeatureVecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3IV2Hddfut1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d657e8de-540e-41ec-f24b-0c9ecb245c8b"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "X_train_cleaned = []\n",
        "for review in X_train:\n",
        "    X_train_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
        "trainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\n",
        "print(\"Training set : %d feature vectors with %d dimensions\" %trainVector.shape)\n",
        "\n",
        "\n",
        "# Get feature vectors for validation set\n",
        "X_test_cleaned = []\n",
        "for review in X_test:\n",
        "    X_test_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
        "testVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\n",
        "print(\"Validation set : %d feature vectors with %d dimensions\" %testVector.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Training set : 40000 feature vectors with 300 dimensions\n",
            "Validation set : 10000 feature vectors with 300 dimensions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Upi6vufut1"
      },
      "source": [
        "# Random Forest Classifer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3EdKCmRfut1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379d221a-8fb1-4480-8f35-47849def1398"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=1000)\n",
        "rf.fit(trainVector, y_train)\n",
        "predictions = rf.predict(testVector)\n",
        "modelEvaluation(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on validation set: 0.8450\n",
            "\n",
            "AUC score : 0.8451\n",
            "\n",
            "Classification report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.84      5035\n",
            "           1       0.83      0.86      0.85      4965\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.85      0.85      0.84     10000\n",
            "weighted avg       0.85      0.84      0.84     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            " [[4164  871]\n",
            " [ 679 4286]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4y8iiggfut2"
      },
      "source": [
        "## LSTM\n",
        "<br>\n",
        "\n",
        "**Step 1 : Prepare X_train and X_test to 2D tensor.**\n",
        "    \n",
        "**Step 2 : Train a simple LSTM (embeddign layer => LSTM layer => dense layer).**\n",
        "    \n",
        "**Step 3 : Compile and fit the model using log loss function and ADAM optimizer.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cup2N2jefut2"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras import backend as K\n",
        "from keras.layers.embeddings import Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74vAMevofut2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b107cd-088b-4ff7-ae21-37c370eaa2ee"
      },
      "source": [
        "top_words = 40000 \n",
        "maxlen = 200 \n",
        "batch_size = 62\n",
        "nb_classes = 4\n",
        "nb_epoch = 6\n",
        "\n",
        "\n",
        "# Vectorize X_train and X_test to 2D tensor\n",
        "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
        "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "# one-hot encoding of y_train and y_test\n",
        "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
        "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "print('X_train shape:', X_train_seq.shape)\n",
        "print(\"========================================\")\n",
        "print('X_test shape:', X_test_seq.shape)\n",
        "print(\"========================================\")\n",
        "print('y_train shape:', y_train_seq.shape)\n",
        "print(\"========================================\")\n",
        "print('y_test shape:', y_test_seq.shape)\n",
        "print(\"========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (40000, 200)\n",
            "========================================\n",
            "X_test shape: (10000, 200)\n",
            "========================================\n",
            "y_train shape: (40000, 4)\n",
            "========================================\n",
            "y_test shape: (10000, 4)\n",
            "========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDjiJ-Qnfut2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad07bc0-5e2c-4f18-9db4-eeebd306974f"
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Embedding(top_words, 128))\n",
        "model1.add(LSTM(128))\n",
        "model1.add(Dropout(0.2)) \n",
        "model1.add(Dense(nb_classes))\n",
        "model1.add(Activation('softmax'))\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 128)         5120000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 516       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 5,252,100\n",
            "Trainable params: 5,252,100\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOX4iox4fut3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60929255-0c77-466c-8b1f-ebb3eb8b45bd"
      },
      "source": [
        "model1.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "\n",
        "# Model evluation\n",
        "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
        "print('Test loss : {:.4f}'.format(score[0]))\n",
        "print('Test accuracy : {:.4f}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "646/646 [==============================] - 78s 69ms/step - loss: 0.3367 - accuracy: 0.6167\n",
            "Epoch 2/6\n",
            "646/646 [==============================] - 45s 70ms/step - loss: 0.1248 - accuracy: 0.9009\n",
            "Epoch 3/6\n",
            "646/646 [==============================] - 45s 69ms/step - loss: 0.0705 - accuracy: 0.9503\n",
            "Epoch 4/6\n",
            "646/646 [==============================] - 45s 69ms/step - loss: 0.0416 - accuracy: 0.9741\n",
            "Epoch 5/6\n",
            "646/646 [==============================] - 45s 69ms/step - loss: 0.0247 - accuracy: 0.9858\n",
            "Epoch 6/6\n",
            "646/646 [==============================] - 44s 69ms/step - loss: 0.0139 - accuracy: 0.9927\n",
            "162/162 [==============================] - 2s 8ms/step - loss: 0.2219 - accuracy: 0.8793\n",
            "Test loss : 0.2219\n",
            "Test accuracy : 0.8793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z4e8QuJ0kTz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZJbbbWOfut3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a90ad30-0b8b-4080-9d99-5b6781ed7661"
      },
      "source": [
        "len(X_train_seq),len(y_train_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 40000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c32ymsAjfut4"
      },
      "source": [
        "## LSTM with Word2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W-K-4Yjfut4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef8be02-c8f6-40df-802c-f4bfd7f9ebaf"
      },
      "source": [
        "w2v = Word2Vec.load(\"w2v_300features_10minwordcounts_10context\")\n",
        "\n",
        "embedding_matrix = w2v.wv.syn0 \n",
        "print(\"Shape of embedding matrix : \", embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of embedding matrix :  (25087, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27d8qbF2fut4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795b272d-8951-423a-c740-c9e70e18390e"
      },
      "source": [
        "top_words = embedding_matrix.shape[0] #4016 \n",
        "maxlen = 300 \n",
        "batch_size = 62\n",
        "nb_classes = 4\n",
        "nb_epoch = 7\n",
        "\n",
        "\n",
        "# Vectorize X_train and X_test to 2D tensor\n",
        "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_seq1 = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
        "X_test_seq1 = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "# one-hot encoding of y_train and y_test\n",
        "y_train_seq1 = np_utils.to_categorical(y_train, nb_classes)\n",
        "y_test_seq1 = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "print('X_train shape:', X_train_seq1.shape)\n",
        "print(\"========================================\")\n",
        "print('X_test shape:', X_test_seq1.shape)\n",
        "print(\"========================================\")\n",
        "print('y_train shape:', y_train_seq1.shape)\n",
        "print(\"========================================\")\n",
        "print('y_test shape:', y_test_seq1.shape)\n",
        "print(\"========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (40000, 300)\n",
            "========================================\n",
            "X_test shape: (10000, 300)\n",
            "========================================\n",
            "y_train shape: (40000, 4)\n",
            "========================================\n",
            "y_test shape: (10000, 4)\n",
            "========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Goxyb4Ofut4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00f3d5d-a943-46aa-bbec-eda54b6c7bbb"
      },
      "source": [
        "len(X_train_seq1),len(y_train_seq1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 40000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6qiJgZQfut4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786ba85a-f362-4caa-d51f-63ee88f7005a"
      },
      "source": [
        "embedding_layer = Embedding(embedding_matrix.shape[0], #4016\n",
        "                            embedding_matrix.shape[1], #300\n",
        "                            weights=[embedding_matrix])\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(embedding_layer)\n",
        "model2.add(LSTM(128))\n",
        "model2.add(Dropout(0.2)) \n",
        "model2.add(Dense(nb_classes))\n",
        "model2.add(Activation('softmax'))\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 300)         7526100   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 516       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 7,746,264\n",
            "Trainable params: 7,746,264\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGwd0WPpfut5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de66644-4ddc-4e14-9233-55246a4dc6c3"
      },
      "source": [
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model2.fit(X_train_seq1, y_train_seq1, batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "\n",
        "# Model evaluation\n",
        "score = model2.evaluate(X_test_seq1, y_test_seq1, batch_size=batch_size)\n",
        "print('Test loss : {:.4f}'.format(score[0]))\n",
        "print('Test accuracy : {:.4f}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "646/646 [==============================] - 74s 112ms/step - loss: 0.3369 - accuracy: 0.6155\n",
            "Epoch 2/7\n",
            "646/646 [==============================] - 72s 112ms/step - loss: 0.1525 - accuracy: 0.8808\n",
            "Epoch 3/7\n",
            "646/646 [==============================] - 72s 112ms/step - loss: 0.0848 - accuracy: 0.9383\n",
            "Epoch 4/7\n",
            "646/646 [==============================] - 72s 111ms/step - loss: 0.0465 - accuracy: 0.9702\n",
            "Epoch 5/7\n",
            "646/646 [==============================] - 72s 111ms/step - loss: 0.0285 - accuracy: 0.9836\n",
            "Epoch 6/7\n",
            "646/646 [==============================] - 72s 111ms/step - loss: 0.0159 - accuracy: 0.9919\n",
            "Epoch 7/7\n",
            "646/646 [==============================] - 72s 111ms/step - loss: 0.0073 - accuracy: 0.9960\n",
            "162/162 [==============================] - 3s 14ms/step - loss: 0.2631 - accuracy: 0.8785\n",
            "Test loss : 0.2631\n",
            "Test accuracy : 0.8785\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}